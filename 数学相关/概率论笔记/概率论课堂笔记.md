# 泊松分布
事实上二项分布的极限便是泊松分布
$\text{即}b(k;n,p_{n})\rightarrow \frac{\lambda^k}{k!}e^{-\lambda}$
证明略

## 泊松分布的产生机制
### 柯西引理
$\text{若}f\text{连续（或单调）}且对于任意的x,y有$
$f(x)f(y)=f(x+y)$
$则f(x)=a^x$
考虑证明$x\in Q$时成立，再利用连续性即可

### 泊松过程
假定一个分布具有以下三个性质：
1. 平稳性（平移不便性）：在一段时间内发生的事件只与时间间隔长度相关而与起始无关
2. 独立增量性（无后效性）：在一段时间内发生k个事件与这之前发生的事件独立
3. 普通性：在充分小的时间间隔内，最多发生一次事件
则该过程为泊松过程，发生的次数满足泊松分布
证明如下：
$$
\begin{gather} 
\text{记}P_{k}(t)为时间t内发生k次事件的概率\\
从而我们有P_{k}(t+\Delta t)=\sum_{i=0}^{k} P_{k-i}(t)P_{i}(\Delta t) \\
先考虑k=0的情况，此时P_{0}(t+\Delta t)=P_{0}(t)P_{0}(\Delta t)由柯西引理知P_{0}(t)=e^{ t\ln a } \\
接下来由普通性，令\Delta t\rightarrow 0,便得到P_{k}(t+\Delta t)=P_{k}(t)P_{0}(\Delta t)+P_{k-1}(t)P_{1}(\Delta t) \\
=P_{k}(t)e^{ \Delta t\ln a }+P_{k-1}(t)(1-e^{ \Delta t\ln a }) \\
稍加整理得到 \frac{P_{k}(t+\Delta t)-P_{k}(t)}{\Delta t}=\frac{P_{k-1}(t)-P_{k}(t)}{\Delta t} (1-e^{ \Delta t\ln a }) \\
即\frac{dP_{k}}{dt} = -\ln a(P_{k-1}-P_{k}) (k=1,2\dots)与P_{0}=e^{ t\ln a }\\
我们得到了一个微分方程组，解它便得到P_{k}(t)= \frac{(\lambda t)^k}{k!}e^{ -\lambda t}这里令\ln a=-\lambda
\end{gather} 
$$
$关于微分方程组的一个解法，两边乘上e^{ \lambda t }得：$
$e^{ \lambda t }\frac{dP_{k}}{dt}=\lambda e^{ \lambda t }(P_{k-1}-P_{k})$;
$也即\frac{d}{dt}(e^{ \lambda t }P_{k})=\lambda e^{ \lambda t }P_{k-1}$;
$从而\frac{d^k}{dt^k}(e^{ \lambda t }P_{k}(t))=\lambda ^ke^{ \lambda t }P_{0}(t)=\lambda^k$;
$然后便可逐次解得P_{k}(t)$

思考：
1. 考虑将时间往前走一小步来得到一个方程
2. 注意计算一些特殊的情况，如上文中的$P_{0}(t)$
3. 解ODE时考虑一些凑微分的策略（或者硬做其实也行)

# 随机变量
## 随机变量的定义：
我们可以把一些随机现象的结果和数值联系起来（哪怕它的结果看起来和数值无关，如可以把抛硬币的结果用0和1来表示）
$若随机试验E的每一个可能结果\omega \in \Omega可以用一个数\eta来表示，且随试验的结果变化，则\eta为样本点的一个函数，且称其为随机变量$
$对于一个随机变量\eta,它是定义在样本空间上的实值函数，随实验不同而取得不同的值$
$同时我们还要关心随机变量\eta取值的概率规律$
>[!info]
>***随机变量其实不是变量，而是函数***
*Define:随机变量*
$设\eta(\omega)是定义在概率测度空间(\Omega,\Phi,P)上的单值函数，如果\eta\text{是Borel可测}$[^1]${的，那么我们称其为随机变量}$
$同时我们把P(\eta(\omega)\in B)称作\eta(\omega)的概率分布$

根据实变函数的集合，$\eta$为Borel可测的等价于$\eta^{-1}((-\infty,x))\in \Phi$
从而我们给出分布函数的定义
>[!info]
>称$F(x)=P\{\xi(\omega)<x\}$为随机变量$\xi(\omega)$的累计分布函数
>由测度论的方法可知分布函数可以唯一决定概率分布



[^1]: 关于Borel可测，参考[[实分析笔记（可能会咕咕）]]

## 分布函数的性质
>[!info]
> 1.单调性：若$a<b$，则$F(a)\leq F(b)$
> 2.有界性：$0<F(x)<1,F(-\infty)=0,F(+\infty)=1$
> 3.左连续性:$F(x-0)=F(x)$[^2]

证明如下：
$$
\begin{gather}
1.F(b)-F(a)=P(\xi<b)-P(\xi<a)=P(a\leq \xi<b)\geq0 \\
2.由F的定义立刻得到0<F(x)<1,而1=P(\Omega)=P\left( \bigcup[n,n+1) \right)= \\
\sum F(n+1)-F(n)=\lim_{ n \to \infty } F(n)-\lim_{ m \to -\infty }F(m) \\
在利用F(x)的单调有界性便得到F(-\infty)=0,F(\infty)=1  \\
3.任取x_{n}\to x且x_{n}<x,则F(x)=P\left(\bigcup(\xi<x_{n})\right)=\lim_{ n \to \infty } P(\xi<x_{n})=F(x-0)
\end{gather}
$$
可以用分布函数来计算关于随机变量的概率：
1. $P(\xi\leq a)=P\left( \bigcap \xi<a+\frac{1}{n} \right)=F(a+0)$
2. $P(\xi=a)=P(\xi\leq a)-P(\xi<a)=F(a+0)-F(a)$
3. $P(\xi>a)=P(\Omega)-P(x\leq a)=1-F(a+0)$
4. $P(\xi\geq a)=1-F(a)$

## 两类重要的随机变量
### 离散随机变量
定义：若随机变量$\xi$的全部可能取值是有限多个或者可列多个，则称$\xi$是离散随机变量
	设${x_{i}}$为离散随机变量$\xi$的所有可能取值，且$p(x_{i})$是$\xi\text{取}x_{i}\text{的概率}$
	那么我们称{$p(x_{i}),i=1,2,3,\dots$}为$\xi$的概率分布
概率分布和分布函数可以相互求得
#### 离散型随机变量及其概率分布的例子
- 退化分布（单点分布）
- 伯努利分布（01分布）
- 二项分布：可以分解为独立的伯努利分布
- 超几何分布：（N很大时可用二项分布近似）
- 泊松分布：（二项分布的极限）
- 几何分布，几何分布具有无记忆性[^3]
- 帕斯卡分布（负二项分布[^4]）:可以分解为独立的几何分布，计算数学期望和方差时有用

### 连续型随机变量
定义：设$F(x)$是随机变量$\xi$的分布函数，若存在非负可积函数$p(x)$使得对任意的实数$x$有：
	$F(x)=\int_{-\infty}^{x}p(t)  \, dt$
则称$\xi$为连续随机变量，称$p(x)$为概率密度函数，简称概率密度或密度函数（pdf)
#### 概率密度函数的性质
- 非负性：$p(x)>0$
- 规范性:$\int_{-\infty}^{\infty}p(x)  \, dx=1$

由牛顿-莱布尼兹公式：我们有：$P(a\leq \xi<b)=F(b)-F(a)=\int_{a}^{b} p(x) \, dx$
事实上，由积分中值定理$p(x)dx$可以看成$\xi在x点附近的概率$
由实分析的知识，我们可以知道$P$在0测集上的取值为0（如在实数轴上任取一点，该点为有理数的概率为0）

分布函数，和概率密度函数都是对随机变量的完整刻画，但密度函数在图形上更优胜

#### 常见的连续随机变量
- 均匀(uniform)分布:$$
p(x)=\begin{cases}0 & x\notin[a,b] \\
\frac{1}{b-a} & x \in[a,b]
\end{cases}
$$
- 正态(normal)分布：概率密度函数为： $$
p(x)=\frac{1}{\sqrt{ 2\pi }\sigma}e^{ - \frac{(x-\mu)^2}{2\sigma^2} }
$$
简记为$N(\mu,\sigma^2)，当\mu=0,\sigma=1时称作$标准正态分布，密度函数与分布函数记作$\phi(x)与\Phi(x)$
	 特点：
		 1. $p(x)关于\mu对称$
		 2. $p(x)在x=\mu处取得极值$
		 3. $\sigma越大分布约平坦$
	标准化：若$X$～$N(\mu,\sigma^2)$,则$Y\text{～}N(0,1)$，使用定积分的换元法易证
	3$\sigma$原则：若$X$～$N(\mu,\sigma^2)$,则：
		$P(|X-\mu|<\sigma)\approx68.27\%$
		$P(|X-\mu|<2\sigma)\approx 95.45\%$
		$P(|X-\mu|<3\sigma)\approx99.73\%$
		可以看出$X$几乎总是落在$(\mu-3\sigma,\mu+3\sigma)$之内
	中心极限定理告诉我们，自然界中的大部分分布都服从正态分布

- 指数分布:简记为Exp$(\lambda)$,其概率密度函数为:
$$
p(x)=\begin{cases}
\lambda e^{-\lambda x}, & x\geq0 \\
0, & x < 0
\end{cases}
$$ 其分布函数为:
$$
F(x)=\begin{cases}
1-e^{ -\lambda x }, & x\geq 0 \\
0, & x<0
\end{cases}
$$指数分布同样具有无记忆性[^3]
指数分布和泊松过程具有重要的关系:应该指出,参数为$\lambda t$的泊松过程等待第一个事件发生的时刻$\xi$服从参数为$\lambda$的指数分布
- 埃尔朗分布: 若用$W_{r}来记参数为\lambda t泊松过程中第r个事件发生的时刻,并以F(x)来记W_{r}的分布函数$
可以得到$F'(x)=\frac{\lambda}{\Gamma(r)}t^{r-1}e^{ -\lambda t }=p(x)$
这被称作埃尔朗分布
事实上可以把$r的取值推广到正实数$
从而得到$\Gamma分布$简记为$\Gamma(r,\lambda)$
与帕斯卡和几何分布的关系类似,埃尔朗分布也可以看作指数分布的和

### 随机向量
有时试验的结果可能并不只能用一个数字来描述，可能对于一个样本点$\omega$，它对应的试验结果是一个向量$(\xi_{1}(\omega),\xi_{2}(\omega),\dots \xi_{n}(\omega))$

## 联合分布函数：
称n元函数
$F(x_{1},x_{2},\dots,x_{n})=P(\xi_{1}<x_{1},\xi_{2}<x_{2},\dots \xi_{n}<x_{n})$
为随机向量$\xi(\omega)=(\xi_{1}(\omega)+\xi_{2}(\omega)+\dots+\xi_{n}(\omega)的联合分布函数$
有了分布函数，我们便可以计算事件的概率，如n=2时：
$P(a_{1}\leq \xi<b_{1}),a_{2}\leq \xi<b_{2}=F(b_{1},b_{2})+F(a_{1},a_{2})-F(b_{1},a_{2})-F(a_{1},b_{2})$

类似于一元的场合，我们可以得到一些多元分布函数的性质：
- 单调性：关于每个变元都是单调不减函数;
- $F(x_{1},x_{2},\dots,-\infty, \dots,x_{n})=0$
  $F(+\infty,+\infty, \dots,+\infty)=1;$
- 关于每个变量左连续
- 概率不小于0,如n=2时，我们有:$F(b_{1},b_{2})+F(a_{1},a_{2})-F(b_{1},a_{2})-F(a_{1},b_{2})\geq 0$

一些常见的分布：
- 多项分布：
	若每次试验有n种可能，且概率为$p_{i}$,那么
	$P(\xi_{1}=k_{1},\xi_{2}=k_{2}, \dots \xi_{r}=k_{r})= \frac{n!}{k_{1}!k_{2}!\dots k_{r}!}p_{1}^{k_{1}}p_{2}^{k_{2}}\dots p_{r}^{k_{r}}$
- 多元超几何分布：
- 均匀分布： $$
p(x_{1},x_{2},\dots,x_{n})=\begin{cases}
\frac{1}{S}, & (x_{1},x_{2},\dots,x_{n})\in G \\
0, &(x_{1},x_{2},\dots,x_{n})\notin G 
\end{cases}
$$
- 多元正态分布：
	$p(x)=\frac{1}{(2\pi)^{n/2}(\det\Sigma)^{1/2}}\exp\left[ -\frac{1}{2}(x-\mu)\Sigma^{-1}(x-\mu)^{T} \right]$
	其中$\Sigma=(\sigma_{ij})为n阶正定对称矩阵，\mu=(\mu_{1},\mu_{2},\dots \mu_{n})$

## 边缘分布：
若我们知道联合分布函数(或分布列)时，如何求得单个变量的分布函数(或分布列)呢？
先考虑二元的情况：
事实上我们有
$p(x_{i})=\sum_{j=1}^{n} p(x_{i},y_{j})$

| $\mu$\ $\xi$   | $y_{1}$          | $y_{2}$          | ... | $y_{n}$          | $p_{1}(x_{i})$ |
| -------------- | ---------------- | ---------------- | --- | ---------------- | -------------- |
| $x_{1}$        | $p(x_{1},y_{1})$ | $p(x_{1},y_{2})$ |     | $p(x_{1},y_{n})$ |                |
| $x_{2}$        | $p(x_{2},y_{1})$ | $p(x_{2},y_{2})$ |     | $p(x_{2},y_{n})$ |                |
| ...            |                  |                  |     |                  |                |
| $x_{m}$        | $p(x_{m},y_{1})$ | $p(x_{m},y_{2})$ |     | $p(x_{m},y_{n})$ |                |
| $p_{2}(y_{i})$ |                  |                  |     |                  |                |
可以看出来$p(x_{i})$其实刚好是联合分布列边缘上的元素(此即为边缘分布得名的原因)
而对于连续的情况，我们可以定义两个边缘分布：
$F_{1}(x)=P(\xi<x,\mu<+\infty)=F(x,+\infty)$
$F_{2}(x)=F(+\infty,y)$
竟然如此，那我们可以定义一个密度函数$p(x,y)$,使得
$F(x,y)=\int_{-\infty}^{x}  \int_{-\infty}^{y} p(u,v) \, du\, dv$
这样子我们可以知道$(\xi,\mu)落在区域D上的概率为\iint_{D}p(x,y)d\sigma$
且$\frac{ \partial^{2}F }{ \partial x \partial y}=p(x,y)$
同时对于边缘分布，我们有:$p_{1}(x)=\int_{-\infty}^{\infty} p(x,y) \, dy$
	                $p_{2}(y)=\int_{-\infty}^{\infty} p(x,y) \, dx$


[^2]: 若在定义分布函数时将$<$改为$\leq$则改为右连续性

[^3]: 无记忆性：
$P\{\eta>m+n|\eta>m\}=P\{\eta>n\},m>0,n>0$
有趣的是，在离散分布中，只有几何分布是无记忆性的
同样的,在连续分布中,只有指数分布是无记忆性的
[^4]:负二项分布去除了帕斯卡分布中r为正整数的要求，改为正实数
